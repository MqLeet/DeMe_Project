<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Decouple-Then-Merge: Finetune Diffusion Models as Multi-Task Learning</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Decouple-Then-Merge: <br> Finetune Diffusion Models as Multi-Task Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://mqleet.github.io/" target="_blank">Qianli Ma</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://nics-effalg.com/ningxuefei/" target="_blank">Xuefei Ning</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://shenqildr.github.io/" target="_blank">Dongrui Liu</a><sup>3</sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://www.ustcnewly.com/" target="_blank">Li Niu</a><sup>1,4&#8224</sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="http://www.zhanglinfeng.tech/" target="_blank">Linfeng Zhang</a><sup>1&#8224</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup>
                      Shanghai Jiao Tong University &nbsp;&nbsp;
                      <sup>2</sup>
                      Tsinghua University &nbsp;&nbsp;
                      <br>
                      <sup>3</sup>
                      Shanghai AI Laboratory &nbsp;&nbsp;
                      <sup>4</sup>
                      miguo.ai &nbsp;&nbsp;
                      <br>CVPR 2025</span>
                    <span class="eql-cntrb"><small><br><sup>&#8224</sup>Denotes Corresponding Authors</small></small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="static/pdfs/CVPR25_DeMe_main.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/CVPR25_DeMe_supp.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/MqLeet/DeMe" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2410.06664" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->


<!-- teaser -->
<section class="hero teaser">
  <div class="container is-max-desktop" style="width: 60%; max-width: none;">
      <div class="hero-body">
            <img src="static/images/cvpr2025_top1.png" style="width:100%; margin-bottom:10px" alt="Teaser."/>
      <p style="margin-top: 0;">
        Figure 1: (a) Cosine similarity between gradients at different timesteps on CIFAR10 & distribution
        of gradients similarity in t ∈ [0, 1000] and t ∈ [0, 250]. Non-adjacent timesteps have low similarity,
        indicating conflicts during their training. In contrast, adjacent timesteps have similar gradients.
        (b) & (c): Comparison between the traditional and our training paradigm: The previous paradigm
        trains one diffusion model on all timesteps, leading to conflicts in different timesteps. Our method
        addresses this problem by decoupling the training of diffusion models in N different timestep ranges.     
       </p>
    </div>
  </div>
</section>
<!-- End teaser -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Diffusion models are trained by learning a sequence of models that reverse each step of noise corruption. Typically, the model parameters are fully shared across multiple timesteps to enhance training efficiency. However, since the denoising tasks differ at each timestep, the gradients computed at different timesteps may conflict, potentially degrading the overall performance of image generation. To solve this issue, this work proposes a <strong>De</strong>couple-then-<strong>Me</strong>rge (<strong>DeMe</strong>) framework, which begins with a pretrained model and finetunes separate models tailored to specific timesteps. We introduce several improved techniques during the finetuning stage to promote effective knowledge sharing while minimizing training interference across timesteps. Finally, after finetuning, these separate models can be merged into a single model in the parameter space, ensuring efficient and practical inference. Experimental results show significant generation quality improvements upon 6 benchmarks including Stable Diffusion on COCO30K, ImageNet1K, PartiPrompts, and DDPM on LSUN Church, LSUN Bedroom, and CIFAR10. Code is available at <a href="https://github.com/MqLeet/DeMe" target="_blank">GitHub</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Method Overview -->
<section class="hero teaser">
  <div class="container is-max-desktop" style="width: 70%; max-width: none;">
      <div class="hero-body">
            <img src="static/images/cvpr2025_method.png" style="width:100%; margin-bottom:10px" alt="Teaser."/>
      <p style="margin-top: 0;">
        Figure 2: Pipeline of our framework. The following training techniques are incorporated into the
        finetuning process: <strong>Consistency loss</strong> preserves the original knowledge of diffusion models learned
        at all timesteps by minimizing the difference between pre-finetuned and post-finetuned diffusion
        models. <strong>Probabilistic sampling strategy</strong> samples from both the corresponding and other timesteps
        with different probabilities, helping the diffusion model overcome forgetting knowledge from other
        timesteps. <strong>Channel-wise projection</strong> enables the diffusion model to directly capture the feature
        difference in channel dimension. <strong>Model merging scheme</strong> merges the parameters of all the finetuned
        models into one unified model to promote the knowledge sharing across different timestep ranges.
        </p>
      </p>
    </div>
  </div>
</section>
<!-- End Method Overview -->




<!-- Image carousel -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="width: 70%; max-width: none;">
      <h2 class="title is-3">Qualitative Results on Text-to-Image Generation</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/cvpr2025_quality1.png"  alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison between our method and the original Stable Diffusion on various prompts.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/cvpr2025_quality2.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison between our method and the original Stable Diffusion on various prompts.
        </h2>
      </div>
  </div>
</div>
</div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="width: 70%; max-width: none;">
      <h2 class="title is-3">Qualitative Results on Unconditional Image Generation</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/cvpr2025_quality3.png"  alt="Unconditional generation results"/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison between our method and the original Stable Diffusion on various prompts.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/cvpr2025_quality4.png"  alt="Unconditional generation results"/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison between our method and the original Stable Diffusion on various prompts.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="width: 70%; max-width: none;">
      <h2 class="title is-3">Quantitative Results</h2>
      <!-- 使用普通div进行纵向排列 -->
      <div class="content" style="display: flex; flex-direction: column; gap: 2rem;">
        
        <!-- 第一张图片 -->
        <div>
          <img src="static/images/cvpr_table1.png" class="image" style="width:80%; margin-left:100px" alt="Table 1: Quantitative results on text-to-image generation"/>
          <h2 class="subtitle has-text-centered">
            Table 1: Quantitative results on text-to-image generation
          </h2>
        </div>
        
        <!-- 第二张图片 -->
        <div>
          <img src="static/images/cvpr_table2.png" class="image" style="width:80%; margin-left:100px" alt="Table 2: Quantitative results on unconditional image generation"/>
          <h2 class="subtitle has-text-centered">
            Table 2: Quantitative results on unconditional image generation
          </h2>
        </div>

      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="width: 70%; max-width: none;">
      <h2 class="title is-3">Why DeMe works?</h2>
      <!-- 使用普通div进行纵向排列 -->
      <div class="content" style="display: flex; flex-direction: column; gap: 2rem;">
        
        <!-- 第一张图片 -->
        <div>
          <img src="static/images/cvpr_visz1.png" class="image" style="width:80%; margin-left:100px" alt="Table 1: Quantitative results on text-to-image generation"/>
          <p style="margin-top: 0;">
            Loss landscape of the pretrained diffusion model in different timestep ranges on CIFAR10. We use dimension reduction methods
            to handle high-dimensional neural networks. Contour line density reflects the frequency of loss variations (i.e., gradients), with blue
            representing low loss and red representing high loss. The pretrained model resides at the critical point (with zero gradients) with sparse
            contour lines for the overall timesteps t ∈ [0, 1000), but when the training process is decoupled, it tends to be located in regions with
            densely packed contour lines, suggesting that there still exists gradients that enable pretrained model to escape from the critical point.
            </p>

        </div>
        
        <!-- 第二张图片 -->
        <div>
          <img src="static/images/cvpr_visz2.png" class="image" style="width:80%; margin-left:100px" alt="Table 2: Quantitative results on unconditional image generation"/>
          <p style="margin-top: 0;">
            Loss landscape for applying task vectors. The optimal model parameters are neither the pretrained one nor the finetuned one,
            but lie within the plane spanned by the task vectors computed in Sec. 3.3. We utilize the pretrained and two finetuned model parameters
            to obtain the two task vectors, respectively. We compute an orthonormal basis from the plane spanned by the task
            vectors. Axis denotes the movement direction in the parameter space.
          </p>
        </div>

      </div>
    </div>
  </div>
</section>



<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">

      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">

            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">

            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">

            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\

            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code> @InProceedings{ma2024decouple,
        title={Decouple-Then-Merge: Towards Better Training for Diffusion Models},
        author={Ma, Qianli and Ning, Xuefei and Liu, Dongrui and Niu, Li and Zhang, Linfeng},
        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
        year={2025}
        }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
